

ğŸš€ PanScience Innovations â€“ LLM Specialist Assignment (RAG Pipeline)

This repository contains the solution for the PanScience LLM Specialist Assignment â€” a Retrieval-Augmented Generation (RAG) pipeline. It enables users to upload documents, processes them using embeddings, stores them in a FAISS vector DB, and answers user queries contextually using an LLM (OpenAI or Gemini).

â¸»

ğŸ“‘ Table of Contents
	â€¢	âœ¨ Features
	â€¢	ğŸ”§ Requirements
	â€¢	âš™ï¸ Setup and Installation
	â€¢	ğŸ“Œ Prerequisites
	â€¢	ğŸ³ Local Setup (Docker)
	â€¢	ğŸ§ª Manual Setup (Dev)
	â€¢	ğŸ§ª API Usage
	â€¢	ğŸ§  LLM Configuration
	â€¢	âœ… Testing
	â€¢	ğŸš€ Deployment Notes
	â€¢	ğŸ“ˆ Evaluation Criteria
	â€¢	ğŸ”® Future Enhancements

â¸»

âœ¨ Features

<details>
<summary><strong>ğŸ“„ Document Ingestion & Processing</strong></summary>


	â€¢	Upload PDF/TXT files (max 20, 100MB each).
	â€¢	Automatically chunked into segments.
	â€¢	Embeddings generated via HuggingFace models.
	â€¢	Stored in a FAISS vector database.
	â€¢	Metadata saved in PostgreSQL.

</details>


<details>
<summary><strong>ğŸ§  Retrieval-Augmented Generation (RAG)</strong></summary>


	â€¢	Accepts natural language queries.
	â€¢	Retrieves top-k most relevant chunks.
	â€¢	Sends content + query to an LLM (OpenAI or Gemini).
	â€¢	Returns a contextual, concise answer.

</details>


<details>
<summary><strong>ğŸ§± API Architecture</strong></summary>


	â€¢	Built using FastAPI.
	â€¢	Exposes endpoints for:
	â€¢	Document Upload
	â€¢	Query Execution
	â€¢	Metadata Retrieval
	â€¢	Query History

</details>


<details>
<summary><strong>ğŸ“¦ Containerized & Tested</strong></summary>


	â€¢	Docker & Docker Compose ready.
	â€¢	Persistent volumes for DB + FAISS.
	â€¢	Unit & integration test script included.

</details>



â¸»

ğŸ”§ Requirements
	â€¢	Docker & Docker Compose (v1.28+)
	â€¢	Python 3.9+ (for manual/local dev)

â¸»

âš™ï¸ Setup and Installation

ğŸ“Œ Prerequisites

Install Docker Desktop or Docker Engine + Docker Compose on your system.

â¸»

ğŸ³ Local Setup (Docker)

git clone https://github.com/your-github-username/pan-rag-assignment.git
cd pan-rag-assignment

Create a .env file:

OPENAI_API_KEY=your_openai_api_key
GEMINI_API_KEY=your_gemini_api_key
LLM_PROVIDER=gemini
DATABASE_URL=postgresql+psycopg2://user:password@metadata_db/panscience_rag_db
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
VECTOR_DB_DIRECTORY=/app/vector_db_data/faiss_index

Then build and start services:

docker-compose up --build -d

The DB tables (documents, chunks, query_logs) are initialized automatically on first run.

Open the API docs:
ğŸ“ http://localhost:8000/docs

â¸»

ğŸ§ª Manual Setup (Dev)

Useful for debugging without Docker.

python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
export $(grep -v '^#' .env | xargs)

Start DB only:

docker-compose up -d metadata_db

Create DB tables:

python -c "from app.models.metadata import Base, engine; Base.metadata.create_all(bind=engine)"

Run the app:

uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload


â¸»

ğŸ“¡ API Usage

ğŸ§­ Swagger UI

http://localhost:8000/docs

â¸»

ğŸ“¤ POST /upload/documents

Upload multiple PDF/TXT files (max: 20 per request, ~100MB each).

Request: multipart/form-data

files: [file1.pdf, file2.txt]

Example Response:

{
  "message": "Document upload process initiated for all files.",
  "results": [
    {
      "document_id": "abc123",
      "filename": "my_resume.pdf",
      "num_chunks": 12,
      "status": "completed"
    }
  ]
}


â¸»

â“ POST /query/ask

Request:

{
  "query": "What is this document about?",
  "top_k": 4
}

Response:

{
  "query": "What is this document about?",
  "response": "The uploaded document is a resume focused on machine learning and AI development."
}


â¸»

ğŸ“„ GET /documents/metadata

Fetch metadata for uploaded documents.

Query params (optional):
	â€¢	skip
	â€¢	limit
	â€¢	status_filter

â¸»

ğŸ“š GET /query/history

Returns recent queries and their generated responses.

â¸»

ğŸ” LLM Configuration

LLM_PROVIDER=gemini  # or openai
GEMINI_API_KEY=your_key_here
OPENAI_API_KEY=your_openai_key_here


â¸»

âœ… Testing

<details>
<summary><strong>ğŸ§ª Run Tests Inside Docker (Recommended)</strong></summary>


	1.	Ensure Docker is up:

docker-compose up --build -d


	2.	Add a test document:

docker cp test_document.pdf pan-rag-assignement-rag_app-1:/app/test_document.pdf


	3.	Run test:

docker exec -it pan-rag-assignement-rag_app-1 bash
python -m app.test_full_pipeline



âœ… Tests document ingestion, FAISS indexing, retrieval, and LLM response.

</details>


<details>
<summary><strong>âš™ï¸ Run Tests Locally (For Devs)</strong></summary>


docker-compose up -d metadata_db
source venv/bin/activate
export $(grep -v '^#' .env | xargs)
python app/test_full_pipeline.py

</details>



â¸»

ğŸš€ Deployment Notes
	â€¢	âœ… Use RDS / Cloud SQL for production-grade Postgres.
	â€¢	âœ… Use Pinecone / Weaviate / Qdrant for scalable vector stores.
	â€¢	âœ… Secure env vars using AWS Secrets Manager, GCP Secret Manager, or Docker Swarm/Kubernetes secrets.
	â€¢	âœ… Deploy with ECS/Fargate, GKE, or Cloud Run for scalability.

â¸»

ğŸ“ˆ Evaluation Criteria Addressed
	â€¢	âœ… Efficient vector retrieval + LLM response
	â€¢	âœ… Modular, scalable architecture (FastAPI + Docker)
	â€¢	âœ… Clean code, organized structure
	â€¢	âœ… Clear documentation and tests

â¸»

ğŸ”® Future Enhancements
	â€¢	DOCX/Markdown support
	â€¢	Semantic chunking
	â€¢	Background ingestion (Celery/Redis)
	â€¢	Auth layer (JWT/OAuth)
	â€¢	Web UI / Streamlit
	â€¢	Metrics + monitoring
	â€¢	Multi-index support (per-user or per-project)

â¸»

ğŸ§¾ 1. GitHub Wiki Starter Structure (Optional but Great for Teams)

You can create a GitHub Wiki with the following structure:

ğŸ—‚ Wiki Structure (Markdown files)
	â€¢	Home.md
Brief intro, architecture diagram, link to README
	â€¢	API.md
Detailed usage of endpoints, with curl/Postman examples
	â€¢	LLM_Configuration.md
Switching between OpenAI/Gemini, setting up .env
	â€¢	Testing.md
Guide on using test_full_pipeline.py, pytest, or Docker tests
	â€¢	Deployment.md
Docker Compose, cloud deployment notes

âœ… How to Add:
	1.	On GitHub repo page â†’ click the â€œWikiâ€ tab.
	2.	Start with Home and link other pages.
	3.	You can copy most of the content from your README sections to populate it.

Want me to generate the .md files for you? Just say yes.

â¸»

ğŸ“˜ 2. mkdocs.yml + Docs Structure for Static Site Docs (like https://docs.streamlit.io)

mkdocs.yml (basic)

site_name: PanScience RAG Pipeline
theme:
  name: material
nav:
  - Home: index.md
  - Setup: setup.md
  - API Usage: api.md
  - Testing: testing.md
  - Deployment: deployment.md
  - LLM Config: llm.md
  - Future Enhancements: future.md

Folder structure:

/docs
  â”œâ”€ index.md
  â”œâ”€ setup.md
  â”œâ”€ api.md
  â”œâ”€ testing.md
  â”œâ”€ deployment.md
  â”œâ”€ llm.md
  â”œâ”€ future.md

Setup:

pip install mkdocs mkdocs-material
mkdocs serve

Want me to generate all those .md files too? Just say yes.

â¸»

ğŸ”— 3. Postman Environment (clickable import)

You already have the collection, now hereâ€™s a simple environment you can import to auto-fill the base URL and LLM provider:

ğŸ“ File: PanScience_RAG_Environment.postman_environment.json

{
  "id": "d9b92e42-fd44-41b1-bfa2-baaa2f1812ab",
  "name": "PanScience RAG Environment",
  "values": [
    {
      "key": "base_url",
      "value": "http://localhost:8000",
      "enabled": true
    },
    {
      "key": "llm_provider",
      "value": "gemini",
      "enabled": true
    }
  ],
  "_postman_variable_scope": "environment",
  "_postman_exported_at": "2025-07-30T10:00:00.000Z",
  "_postman_exported_using": "Postman/10.23.0"
}

âœ… How to Use
	1.	Open Postman
	2.	Go to Environments â†’ Import
	3.	Select the above JSON file
	4.	Use {{base_url}}/upload/documents in your requests

â¸»

