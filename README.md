

ğŸš€ PanScience Innovations â€“ LLM Specialist Assignment (RAG Pipeline)

This repository contains the solution for the PanScience LLM Specialist Assignment â€” a Retrieval-Augmented Generation (RAG) pipeline. It enables users to upload documents, processes them using embeddings, stores them in a FAISS vector DB, and answers user queries contextually using an LLM (OpenAI or Gemini).

â¸»

ğŸ“‘ Table of Contents
	â€¢	âœ¨ Features
	â€¢	ğŸ”§ Requirements
	â€¢	âš™ï¸ Setup and Installation
	â€¢	ğŸ“Œ Prerequisites
	â€¢	ğŸ³ Local Setup (Docker)
	â€¢	ğŸ§ª Manual Setup (Dev)
	â€¢	ğŸ§ª API Usage
	â€¢	ğŸ§  LLM Configuration
	â€¢	âœ… Testing
	â€¢	ğŸš€ Deployment Notes
	â€¢	ğŸ“ˆ Evaluation Criteria
	â€¢	ğŸ”® Future Enhancements

â¸»

âœ¨ Features

<details>
<summary><strong>ğŸ“„ Document Ingestion & Processing</strong></summary>


	â€¢	Upload PDF/TXT files (max 20, 100MB each).
	â€¢	Automatically chunked into segments.
	â€¢	Embeddings generated via HuggingFace models.
	â€¢	Stored in a FAISS vector database.
	â€¢	Metadata saved in PostgreSQL.

</details>


<details>
<summary><strong>ğŸ§  Retrieval-Augmented Generation (RAG)</strong></summary>


	â€¢	Accepts natural language queries.
	â€¢	Retrieves top-k most relevant chunks.
	â€¢	Sends content + query to an LLM (OpenAI or Gemini).
	â€¢	Returns a contextual, concise answer.

</details>


<details>
<summary><strong>ğŸ§± API Architecture</strong></summary>


	â€¢	Built using FastAPI.
	â€¢	Exposes endpoints for:
	â€¢	Document Upload
	â€¢	Query Execution
	â€¢	Metadata Retrieval
	â€¢	Query History

</details>


<details>
<summary><strong>ğŸ“¦ Containerized & Tested</strong></summary>


	â€¢	Docker & Docker Compose ready.
	â€¢	Persistent volumes for DB + FAISS.
	â€¢	Unit & integration test script included.

</details>



â¸»

ğŸ”§ Requirements
	â€¢	Docker & Docker Compose (v1.28+)
	â€¢	Python 3.9+ (for manual/local dev)

â¸»

âš™ï¸ Setup and Installation

ğŸ“Œ Prerequisites

Install Docker Desktop or Docker Engine + Docker Compose on your system.

â¸»

ğŸ³ Local Setup (Docker)

git clone https://github.com/your-github-username/pan-rag-assignment.git
cd pan-rag-assignment

Create a .env file:

OPENAI_API_KEY=your_openai_api_key
GEMINI_API_KEY=your_gemini_api_key
LLM_PROVIDER=gemini
DATABASE_URL=postgresql+psycopg2://user:password@metadata_db/panscience_rag_db
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
VECTOR_DB_DIRECTORY=/app/vector_db_data/faiss_index

Then build and start services:

docker-compose up --build -d

The DB tables (documents, chunks, query_logs) are initialized automatically on first run.

Open the API docs:
ğŸ“ http://localhost:8000/docs

â¸»

ğŸ§ª Manual Setup (Dev)

Useful for debugging without Docker.

python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
export $(grep -v '^#' .env | xargs)

Start DB only:

docker-compose up -d metadata_db

Create DB tables:

python -c "from app.models.metadata import Base, engine; Base.metadata.create_all(bind=engine)"

Run the app:

uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload


â¸»

ğŸ“¡ API Usage

ğŸ§­ Swagger UI

http://localhost:8000/docs

â¸»

ğŸ“¤ POST /upload/documents

Upload multiple PDF/TXT files (max: 20 per request, ~100MB each).

Request: multipart/form-data

files: [file1.pdf, file2.txt]

Example Response:

{
  "message": "Document upload process initiated for all files.",
  "results": [
    {
      "document_id": "abc123",
      "filename": "my_resume.pdf",
      "num_chunks": 12,
      "status": "completed"
    }
  ]
}


â¸»

â“ POST /query/ask

Request:

{
  "query": "What is this document about?",
  "top_k": 4
}

Response:

{
  "query": "What is this document about?",
  "response": "The uploaded document is a resume focused on machine learning and AI development."
}


â¸»

ğŸ“„ GET /documents/metadata

Fetch metadata for uploaded documents.

Query params (optional):
	â€¢	skip
	â€¢	limit
	â€¢	status_filter

â¸»

ğŸ“š GET /query/history

Returns recent queries and their generated responses.

â¸»

ğŸ” LLM Configuration

LLM_PROVIDER=gemini  # or openai
GEMINI_API_KEY=your_key_here
OPENAI_API_KEY=your_openai_key_here


â¸»

âœ… Testing

<details>
<summary><strong>ğŸ§ª Run Tests Inside Docker (Recommended)</strong></summary>


	1.	Ensure Docker is up:

docker-compose up --build -d


	2.	Add a test document:

docker cp test_document.pdf pan-rag-assignement-rag_app-1:/app/test_document.pdf


	3.	Run test:

docker exec -it pan-rag-assignement-rag_app-1 bash
python -m app.test_full_pipeline



âœ… Tests document ingestion, FAISS indexing, retrieval, and LLM response.

</details>


<details>
<summary><strong>âš™ï¸ Run Tests Locally (For Devs)</strong></summary>


docker-compose up -d metadata_db
source venv/bin/activate
export $(grep -v '^#' .env | xargs)
python app/test_full_pipeline.py

</details>



â¸»

ğŸš€ Deployment Notes
	â€¢	âœ… Use RDS / Cloud SQL for production-grade Postgres.
	â€¢	âœ… Use Pinecone / Weaviate / Qdrant for scalable vector stores.
	â€¢	âœ… Secure env vars using AWS Secrets Manager, GCP Secret Manager, or Docker Swarm/Kubernetes secrets.
	â€¢	âœ… Deploy with ECS/Fargate, GKE, or Cloud Run for scalability.

â¸»

ğŸ“ˆ Evaluation Criteria Addressed
	â€¢	âœ… Efficient vector retrieval + LLM response
	â€¢	âœ… Modular, scalable architecture (FastAPI + Docker)
	â€¢	âœ… Clean code, organized structure
	â€¢	âœ… Clear documentation and tests

â¸»

ğŸ”® Future Enhancements
	â€¢	DOCX/Markdown support
	â€¢	Semantic chunking
	â€¢	Background ingestion (Celery/Redis)
	â€¢	Auth layer (JWT/OAuth)
	â€¢	Web UI / Streamlit
	â€¢	Metrics + monitoring
	â€¢	Multi-index support (per-user or per-project)

â¸»