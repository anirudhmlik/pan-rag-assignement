ğŸš€ PanScience Innovations â€“ LLM Specialist Assignment (RAG Pipeline)

This repository contains the solution for the PanScience LLM Specialist Assignment â€” a Retrieval-Augmented Generation (RAG) pipeline. It enables users to upload documents, processes them using embeddings, stores them in a FAISS vector DB, and answers user queries contextually using an LLM (OpenAI or Gemini).

â¸»

ğŸ“ Table of Contents
	â€¢	âœ¨ Features
	â€¢	ğŸ”§ Requirements
	â€¢	âš™ï¸ Setup and Installation
	â€¢	ğŸ“Œ Prerequisites
	â€¢	ğŸ³ Local Setup (Docker)
	â€¢	ğŸ§² Manual Setup (Dev)
	â€¢	ğŸ¥ª API Usage
	â€¢	ğŸ§  LLM Configuration
	â€¢	âœ… Testing
	â€¢	ğŸš€ Deployment Notes
	â€¢	ğŸ“ˆ Evaluation Criteria
	â€¢	ğŸ”® Future Enhancements

â¸»

âœ¨ Features
	â€¢	Upload PDF/TXT files (max 20, 100MB each).
	â€¢	Automatically chunked into segments.
	â€¢	Embeddings generated via HuggingFace models.
	â€¢	Stored in a FAISS vector database.
	â€¢	Metadata saved in PostgreSQL.

	â€¢	Accepts natural language queries.
	â€¢	Retrieves top-k most relevant chunks.
	â€¢	Sends content + query to an LLM (OpenAI or Gemini).
	â€¢	Returns a contextual, concise answer.

	â€¢	Built using FastAPI.
	â€¢	Exposes endpoints for:
	â€¢	Document Upload
	â€¢	Query Execution
	â€¢	Metadata Retrieval
	â€¢	Query History

	â€¢	Docker & Docker Compose ready.
	â€¢	Persistent volumes for DB + FAISS.
	â€¢	Unit & integration test script included.

â¸»

ğŸ”§ Requirements
	â€¢	

â¸»

âš™ï¸ Setup and Installation

ğŸ“Œ Prerequisites

Install Docker Desktop or Docker Engine + Docker Compose on your system.

â¸»

ğŸ³ Local Setup (Docker)

git clone https://github.com/your-github-username/pan-rag-assignment.git
cd pan-rag-assignment

Create a .env file:

OPENAI_API_KEY=your_openai_api_key
GEMINI_API_KEY=your_gemini_api_key
LLM_PROVIDER=gemini
DATABASE_URL=postgresql+psycopg2://user:password@metadata_db/panscience_rag_db
EMBEDDING_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
VECTOR_DB_DIRECTORY=/app/vector_db_data/faiss_index

Then build and start services:

docker-compose up --build -d

The DB tables (documents, chunks, query_logs) are initialized automatically on first run.

Open the API docs:
ğŸ“ http://localhost:8000/docs

â¸»

ğŸ§² Manual Setup (Dev)

Useful for debugging without Docker.

python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
export $(grep -v '^#' .env | xargs)

Start DB only:

docker-compose up -d metadata_db

Create DB tables:

python -c "from app.models.metadata import Base, engine; Base.metadata.create_all(bind=engine)"

Run the app:

uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload


â¸»

ğŸ›° API Usage

ğŸ§½ Swagger UI

http://localhost:8000/docs

â¸»

ğŸ“¤ POST /upload/documents

Upload multiple PDF/TXT files (max: 20 per request, ~100MB each).

Request: multipart/form-data

files: [file1.pdf, file2.txt]

Example Response:

{
  "message": "Document upload process initiated for all files.",
  "results": [
    {
      "document_id": "abc123",
      "filename": "my_resume.pdf",
      "num_chunks": 12,
      "status": "completed"
    }
  ]
}


â¸»

â“ POST /query/ask

Request:

{
  "query": "What is this document about?",
  "top_k": 4
}

Response:

{
  "query": "What is this document about?",
  "response": "The uploaded document is a resume focused on machine learning and AI development."
}


â¸»

ğŸ“„ GET /documents/metadata

Fetch metadata for uploaded documents.

Query params (optional):
	â€¢	skip
	â€¢	limit
	â€¢	status_filter

â¸»

ğŸ“š GET /query/history

Returns recent queries and their generated responses.

â¸»

ğŸ” LLM Configuration

LLM_PROVIDER=gemini  # or openai
GEMINI_API_KEY=your_key_here
OPENAI_API_KEY=your_openai_key_here


â¸»

ğŸ–¥ï¸ Streamlit Interface (Optional UI)

For a visual and user-friendly interface to interact with your RAG pipeline:
	1.	Ensure your Docker container is running:

docker-compose up --build -d


	2.	Access the Streamlit app (inside the container):

docker exec -it pan-rag-assignement-rag_app-1 streamlit run streamlit_app.py --server.port 8501


	3.	Open the UI in your browser:
ğŸ‘‰ http://localhost:8501

The Streamlit interface allows:
	â€¢	Uploading and querying documents interactively
	â€¢	Selecting between OpenAI and Gemini
	â€¢	Viewing source document context for each answer

â¸»

âœ… Testing
	1.	Ensure Docker is up:

docker-compose up --build -d


	2.	Add a test document:

docker cp test_document.pdf pan-rag-assignement-rag_app-1:/app/test_document.pdf


	3.	Run test:

docker exec -it pan-rag-assignement-rag_app-1 bash
python -m app.test_full_pipeline



ğŸ“… Tests document ingestion, FAISS indexing, retrieval, and LLM response.

docker-compose up -d metadata_db
source venv/bin/activate
export $(grep -v '^#' .env | xargs)
python app/test_full_pipeline.py


â¸»

ğŸš€ Deployment Notes
	â€¢	âœ… Use RDS / Cloud SQL for production-grade Postgres.
	â€¢	âœ… Use Pinecone / Weaviate / Qdrant for scalable vector stores.
	â€¢	âœ… Secure env vars using AWS Secrets Manager, GCP Secret Manager, or Docker Swarm/Kubernetes secrets.
	â€¢	âœ… Deploy with ECS/Fargate, GKE, or Cloud Run for scalability.

â¸»

ğŸ“ˆ Evaluation Criteria Addressed
	â€¢	âœ… Efficient vector retrieval + LLM response
	â€¢	âœ… Modular, scalable architecture (FastAPI + Docker)
	â€¢	âœ… Clean code, organized structure
	â€¢	âœ… Clear documentation and tests

â¸»

ğŸ”® Future Enhancements
	â€¢	DOCX/Markdown support
	â€¢	Semantic chunking
	â€¢	Background ingestion (Celery/Redis)
	â€¢	Auth layer (JWT/OAuth)
	â€¢	Web UI / Streamlit
	â€¢	Metrics + monitoring
	â€¢	Multi-index support (per-user or per-project)

â¸»

This project includes:
	â€¢	âœ… A GitHub Wiki with structured guides
	â€¢	âœ… A docs/ folder for generating a documentation site via mkdocs-material
	â€¢	âœ… A Postman collection and environment for testing API endpoints
	â€¢	âœ… A Streamlit-based UI for real-time document querying, upload, and LLM responses
